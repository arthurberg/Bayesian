
# Class 14: Hierarchical Models

## Example 1: Bristol babies

:::{.example #bristol-babies name="Bristol babies" .lizi}

The [Bristol heart scandal](https://en.wikipedia.org/wiki/Bristol_heart_scandal came about after it was discovered that babies were dying at high rates after cardiac surgery when treated at the [Bristol Royal Infirmary](https://en.wikipedia.org/wiki/Bristol_Royal_Infirmary). The comprehensive report [@Kennedy:2001] subsequently led to substantial changes in health service monitoring in the UK. Data from this incident was statistically analyzed in [@Spiegelhalter:2002] and [@Marshall:2007], and includes the table below. The following models are fit: 

* constant-risk model
\[
\begin{split}
X_i&\sim\text{Binomial}(n_i,\theta)\\
\theta&\sim\text{Uniform}(0,1)
\end{split}
\]
* independent parameters model
\[
\begin{split}
X_i&\sim\text{Binomial}(n_i,\theta_i)\\
\theta_i&\sim\text{Uniform}(0,1)
\end{split}
\]
* hierarchical model
\[
\begin{split}
X_i&\sim\text{Binomial}(n_i,\theta_i)\\
\text{logit_i}(\theta)&\sim \mathcal{N}(\mu,\sigma^2)\\
\mu&\sim\text{Uniform}(-100,100)\\
1/\sigma^2&\sim\text{Uniform}(0,100)
\end{split}
\]
Residual analysis of the constant-risk model shows a poor model selection. Shrinkage is observed for the parameters in the hierarchical model.
:::


```{r,tidy=TRUE}
library(tidyverse)
library(kableExtra)
bristol=data.frame(hospital=c("Bristol", "Leicester", "Leeds", "Oxford", "Guys", "Liverpool", "Southampton", "Great Ormond St", "Newcastle", "Harefield", "Birmingham", "Brompton"), operations=c(143,187,323,122,164,405,239,482,195,177,581,301), deaths=c(41,25, 24, 23, 25, 42,24,53,26,25,58,31)) %>% mutate(mortality=deaths/operations) %>% arrange(desc(mortality))

#xtable(bristol,digits=c(0,0,0,0,2))

kbl(bristol) %>% kable_classic_2(full_width=F)
```

### Constant-risk model

```{r, tidy=TRUE}
model= function(){

  ## Likelihood
  for (i in 1:12) {
    y[i] ~ dbin(theta, n[i])
    res[i] <- (y[i] - n[i]*theta)/sqrt(n[i]*theta*(1-theta)) 
    res2[i] <- res[i]*res[i]
    }
  
  ## prior
  theta ~ dunif(0, 1)
  X2.obs <- sum(res2[])

  }


data=list(n = bristol$operations, y=bristol$deaths)

library(R2jags)
fit <- jags(data=data, model=model,parameters.to.save=c("theta","res","X2.obs"), n.chain=2, n.iter=5000, n.thin=1, n.burn=100, DIC=FALSE)
fit.mcmc <- as.mcmc(fit)
summary(fit.mcmc)
```

```{r, fig.width=10}
mat=as.matrix(as.mcmc(fit))
theta.avg=mean(mat[,"theta"])
boxplot(mat[,paste0("res[",1:12,"]")])
abline(h=0)

library("bayesplot")
library("ggplot2")
plot_title <- ggtitle("Posterior distributions  of residuals",
                      "with medians and 95% intervals")
mcmc_areas(mat,
           pars = paste0("res[",1:12,"]"),
           prob = 0.95) + plot_title

```


### Independent-parameters model 

```{r, tidy=TRUE}
model= function(){

  ## Likelihood
  for (i in 1:12) {
    y[i] ~ dbin(theta[i], n[i])
    }
  
  ## priors
  for (i in 1:12) {
    theta[i] ~ dunif(0,1)
    }
  }


data=list(n = bristol$operations, y=bristol$deaths)

fit <- jags(data=data, model=model,parameters.to.save=c("theta"), n.chain=2, n.iter=5000, n.thin=1, n.burn=100, DIC=FALSE)
fit.mcmc <- as.mcmc(fit)
summary(fit.mcmc)
```

```{r, fig.width=10}
mat=as.matrix(as.mcmc(fit))

mat.ind=mat[,paste0("theta[",1:12,"]")]

plot_title <- ggtitle("Posterior distributions of thetas",
                      "with medians and 95% intervals")
mcmc_areas(mat,
           pars = paste0("theta[",1:12,"]"),
           prob = 0.95) + plot_title

```


### Hierarchical model 


```{r, tidy=TRUE}
model= function(){

  ## Likelihood
  for (i in 1:12) {
    y[i] ~ dbin(theta[i], n[i])
    logit(theta[i]) <- logit.theta[i]
    logit.theta[i] ~ dnorm(mu,inv.sigma.squared)
    }
  
  ## priors
  inv.sigma.squared <- 1/pow(sigma,2)
  sigma ~ dunif(0,100)
  mu ~ dunif(-100,100)
  }


data=list(n = bristol$operations, y=bristol$deaths)

fit <- jags(data=data, model=model,parameters.to.save=c("theta","mu","sigma"), n.chain=2, n.iter=5000, n.thin=1, n.burn=100, DIC=FALSE)
fit.mcmc <- as.mcmc(fit)
summary(fit.mcmc)
```


```{r, fig.width=10}
mat=as.matrix(as.mcmc(fit))

mat.hier=mat[,paste0("theta[",1:12,"]")]

plot_title <- ggtitle("Posterior distributions of thetas",
                      "with medians and 95% intervals")
mcmc_areas(mat,
           pars = paste0("theta[",1:12,"]"),
           prob = 0.95) + plot_title

```


```{r, tidy=TRUE}
library(reshape2)
df1=melt(mat.ind) %>% mutate(model="independent")
df2=melt(mat.hier) %>% mutate(model="hierarchical")
df=bind_rows(df1,df2)
df$hospital=c("Bristol", "Leicester", "Leeds", "Oxford", "Guys", "Liverpool", "Southampton", "Great Ormond St", "Newcastle", "Harefield", "Birmingham", "Brompton")[as.numeric(as.factor(df$Var2))]

library(tidyverse)
library(hrbrthemes)
library(viridis)
library(ggridges)

df %>%
  ggplot( aes(y=hospital, x=value,  fill=model)) +
    geom_density_ridges(alpha=0.6) +
    scale_fill_viridis(discrete=TRUE) +
    scale_color_viridis(discrete=TRUE) +
    theme_ipsum() +
    theme(
      panel.spacing = unit(0.1, "lines"),
      strip.text.x = element_text(size = 8)
    ) +
    xlab("") +
    ylab("Posterior Distributions") + 
  geom_vline(xintercept = theta.avg, linetype="dotted", 
                color = "blue", size=1.5)

```
Vertical line is the mean theta parameter from the constant-risk model

## Example 2: James-Stein Baseball

:::{.example #baseball name="James-Stein baseball" .lizi}
See [this vignette](https://mc-stan.org/rstanarm/articles/pooling.html) for more details of this example. The James-Stein baseball data [@Efron:1977] is analyzed using `stan_glm` in the `rstanarm` package. This includes model fits with complete pooling (constant-risk model), no pooling (independent-parameter model), and partial pooling (hierarchical model). 
:::


```{r, tidy=TRUE}
library(rstanarm)
data(bball1970)
bball <- bball1970

#xtable(bball)

kbl(bball) %>% kable_classic_2(full_width=F)

N <- nrow(bball)
K <- bball$AB
y <- bball$Hits
K_new <- bball$RemainingAB
y_new <- bball$RemainingHits

batting_avg <- function(x) print(format(round(x, digits = 3), nsmall = 3), quote = FALSE)
player_avgs <- y / K # player avgs through 45 AB
tot_avg <- sum(y) / sum(K) # overall avg through 45 AB

cat("Player averages through 45 at-bats:\n")
batting_avg(player_avgs)
cat("Overall average through 45 at-bats:\n")
batting_avg(tot_avg)

```

### Complete pooling

```{r, tidy=TRUE}
SEED <- 202
wi_prior <- normal(-1, 1)  # weakly informative prior on log-odds
fit_pool <- stan_glm(cbind(Hits, AB - Hits) ~ 1, data = bball, family = binomial("logit"),
                     prior_intercept = wi_prior, seed = SEED)

invlogit <- plogis  # function(x) 1/(1 + exp(-x))
summary_stats <- function(posterior) {
  x <- invlogit(posterior)  # log-odds -> probabilities
  t(apply(x, 2, quantile, probs = c(0.1, 0.5, 0.9))) 
}

pool <- summary_stats(as.matrix(fit_pool))  # as.matrix extracts the posterior draws
pool <- matrix(pool,  # replicate to give each player the same estimates
               nrow(bball), ncol(pool), byrow = TRUE, 
               dimnames = list(bball$Player, c("10%", "50%", "90%")))
batting_avg(pool)

invlogit <- plogis  # function(x) 1/(1 + exp(-x))
summary_stats <- function(posterior) {
  x <- invlogit(posterior)  # log-odds -> probabilities
  t(apply(x, 2, quantile, probs = c(0.1, 0.5, 0.9)))
}

pool <- summary_stats(as.matrix(fit_pool))  # as.matrix extracts the posterior draws
pool <- matrix(pool,  # replicate to give each player the same estimates
               nrow(bball), ncol(pool), byrow = TRUE,
               dimnames = list(bball$Player, c("10%", "50%", "90%")))
batting_avg(pool)
```

### No pooling

```{r, tidy=TRUE}
fit_nopool <- update(fit_pool, formula = . ~ 0 + Player, prior = wi_prior)
nopool <- summary_stats(as.matrix(fit_nopool))
rownames(nopool) <- as.character(bball$Player)
batting_avg(nopool)
```


### Partial pooling

```{r, tidy=TRUE}
fit_partialpool <-
  stan_glmer(cbind(Hits, AB - Hits) ~ (1 | Player), data = bball,
             family = binomial("logit"),
             prior_intercept = wi_prior, seed = SEED)


# shift each player's estimate by intercept (and then drop intercept)
shift_draws <- function(draws) {
  sweep(draws[, -1], MARGIN = 1, STATS = draws[, 1], FUN = "+")
}
alphas <- shift_draws(as.matrix(fit_partialpool))
partialpool <- summary_stats(alphas)
partialpool <- partialpool[-nrow(partialpool),]
rownames(partialpool) <- as.character(bball$Player)
batting_avg(partialpool)
```

### Observed vs estimated

```{r, tidy=TRUE}
library(ggplot2)
models <- c("complete pooling", "no pooling", "partial pooling")
estimates <- rbind(pool, nopool, partialpool)
colnames(estimates) <- c("lb", "median", "ub")
plotdata <- data.frame(estimates,
                       observed = rep(player_avgs, times = length(models)),
                       model = rep(models, each = N),
                       row.names = NULL)

ggplot(plotdata, aes(x = observed, y = median, ymin = lb, ymax = ub)) +
  geom_hline(yintercept = tot_avg, color = "lightpink", size = 0.75) +
  geom_abline(intercept = 0, slope = 1, color = "skyblue") +
  geom_linerange(color = "gray60", size = 0.75) +
  geom_point(size = 2.5, shape = 21, fill = "gray30", color = "white", stroke = 0.2) +
  facet_grid(. ~ model) +
  coord_fixed() +
  scale_x_continuous(breaks = c(0.2, 0.3, 0.4)) +
  labs(x = "Observed Hits / AB", y = "Predicted chance of hit") +
  ggtitle("Posterior Medians and 80% Intervals")
```

:::{.exercise #rats name="Rat tumors" .prob}
(Tarone, 1982)[@Tarone:1982] provides a data set of tumor incidence in historical control groups of rats; specifically endometrial stromal polyps in female lab rats of type F344 and analyzed in (Gelman, 2021)[@Gelman:2021]. The dataset can be accessed [here](http://www.stat.columbia.edu/~gelman/book/data/rats.asc) or loaded in R with the `rstanarm` package using
```{r}
data(tumors, package = "rstanarm")
```
Fit a hierarchical Bayesian model to the data and reproduce Figure 5.4 on page 113 of (Gelman, 2021)[@Gelman:2021].
:::


## Example 3: ChatGPT

:::{.example #chatgpt name="ChatGPT Hierarchical model" .lizi}
ChatGPT: stan code of a hierarchical model.
:::

Sure! Here is an example of a hierarchical model in Stan. The model assumes that we have a set of data points, y, that come from different groups, and we want to model the variation in the data within each group and the variation in the group means across all groups.

```{stan, output.var="hierarchical_model"}
data {
  int<lower=1> N;  // number of observations
  int<lower=1> K;  // number of groups
  int<lower=1, upper=K> group[N];  // group indicator for each observation
  real y[N];  // observed data
}

parameters {
  real mu0;             // prior mean for group means
  real<lower=0> sigma0;  // prior std. dev. for group means
  real<lower=0> sigma;   // std. dev. of the observations within each group
  vector[K] mu;          // group means
}

model {
  // Priors
  mu ~ normal(mu0, sigma0);  // prior for group means
  sigma ~ cauchy(0, 1);      // weakly informative prior for sigma

  // Likelihood
  for (i in 1:N) {
    y[i] ~ normal(mu[group[i]], sigma);
  }
}
```

```{r}
library(rstan)

y <- c(22, 18, 23, 17, 21, 19, 25, 16, 24, 20, 23, 18, 27, 21, 26, 19, 22, 20, 23, 21)
group <- c(rep(1, 10), rep(2, 10))


data_list <- list(
  N = length(y), 
  K = length(unique(group)), 
  group = group, 
  y = y
)

fit <- sampling(hierarchical_model, data = data_list, chains = 4)
summary(fit)
traceplot(fit)
```


```{stan, output.var="hierarchical_model2"}
data {
  int<lower=1> N;  // number of observations
  int<lower=1> K;  // number of groups
  int<lower=1, upper=K> group[N];  // group indicator for each observation
  real y[N];  // observed data
}

parameters {
  real<lower=0,upper=50> mu0;             // prior mean for group means
  real<lower=0, upper=50> sigma0;  // prior std. dev. for group means
  real<lower=0, upper=50> sigma;   // std. dev. of the observations within each group
  vector[K] mu;          // group means
}

model {
  // Priors
  mu ~ normal(mu0, sigma0);  // prior for group means
  sigma ~ cauchy(0, 1);      // weakly informative prior for sigma

  // Likelihood
  for (i in 1:N) {
    y[i] ~ normal(mu[group[i]], sigma);
  }
}
```

```{r}
fit <- sampling(hierarchical_model2, data = data_list, chains = 4,iter=10^4)
summary(fit)
traceplot(fit)
```

