---
output: html_document
editor_options: 
  chunk_output_type: console
---


# Class 14: Hierarchical Models

## Example 1: Bristol babies

:::{.example #bristol-babies name="Bristol babies" .lizi}

The [Bristol heart scandal](https://en.wikipedia.org/wiki/Bristol_heart_scandal came about after it was discovered that babies were dying at high rates after cardiac surgery when treated at the [Bristol Royal Infirmary](https://en.wikipedia.org/wiki/Bristol_Royal_Infirmary). The comprehensive report [@Kennedy:2001] subsequently led to substantial changes in health service monitoring in the UK. Data from this incident was statistically analyzed in [@Spiegelhalter:2002] and [@Marshall:2007], and includes the table below. The following models are fit: 

* constant-risk model
\[
\begin{split}
X_i&\sim\text{Binomial}(n_i,\theta)\\
\theta&\sim\text{Uniform}(0,1)
\end{split}
\]
* independent parameters model
\[
\begin{split}
X_i&\sim\text{Binomial}(n_i,\theta_i)\\
\theta_i&\sim\text{Uniform}(0,1)
\end{split}
\]
* hierarchical model
\[
\begin{split}
X_i&\sim\text{Binomial}(n_i,\theta_i)\\
\text{logit_i}(\theta)&\sim \mathcal{N}(\mu,\sigma^2)\\
\mu&\sim\text{Uniform}(-100,100)\\
1/\sigma^2&\sim\text{Uniform}(0,100)
\end{split}
\]
Residual analysis of the constant-risk model shows a poor model selection. Shrinkage is observed for the parameters in the hierarchical model.
:::



```r
library(tidyverse)
library(kableExtra)
bristol = data.frame(hospital = c("Bristol", "Leicester", "Leeds",
    "Oxford", "Guys", "Liverpool", "Southampton", "Great Ormond St",
    "Newcastle", "Harefield", "Birmingham", "Brompton"), operations = c(143,
    187, 323, 122, 164, 405, 239, 482, 195, 177, 581, 301), deaths = c(41,
    25, 24, 23, 25, 42, 24, 53, 26, 25, 58, 31)) %>%
    mutate(mortality = deaths/operations) %>%
    arrange(desc(mortality))

# xtable(bristol,digits=c(0,0,0,0,2))

kbl(bristol) %>%
    kable_classic_2(full_width = F)
```

<table class=" lightable-classic-2" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>
 <thead>
  <tr>
   <th style="text-align:left;"> hospital </th>
   <th style="text-align:right;"> operations </th>
   <th style="text-align:right;"> deaths </th>
   <th style="text-align:right;"> mortality </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> Bristol </td>
   <td style="text-align:right;"> 143 </td>
   <td style="text-align:right;"> 41 </td>
   <td style="text-align:right;"> 0.2867133 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Oxford </td>
   <td style="text-align:right;"> 122 </td>
   <td style="text-align:right;"> 23 </td>
   <td style="text-align:right;"> 0.1885246 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Guys </td>
   <td style="text-align:right;"> 164 </td>
   <td style="text-align:right;"> 25 </td>
   <td style="text-align:right;"> 0.1524390 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Harefield </td>
   <td style="text-align:right;"> 177 </td>
   <td style="text-align:right;"> 25 </td>
   <td style="text-align:right;"> 0.1412429 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Leicester </td>
   <td style="text-align:right;"> 187 </td>
   <td style="text-align:right;"> 25 </td>
   <td style="text-align:right;"> 0.1336898 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Newcastle </td>
   <td style="text-align:right;"> 195 </td>
   <td style="text-align:right;"> 26 </td>
   <td style="text-align:right;"> 0.1333333 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Great Ormond St </td>
   <td style="text-align:right;"> 482 </td>
   <td style="text-align:right;"> 53 </td>
   <td style="text-align:right;"> 0.1099585 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Liverpool </td>
   <td style="text-align:right;"> 405 </td>
   <td style="text-align:right;"> 42 </td>
   <td style="text-align:right;"> 0.1037037 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Brompton </td>
   <td style="text-align:right;"> 301 </td>
   <td style="text-align:right;"> 31 </td>
   <td style="text-align:right;"> 0.1029900 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Southampton </td>
   <td style="text-align:right;"> 239 </td>
   <td style="text-align:right;"> 24 </td>
   <td style="text-align:right;"> 0.1004184 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Birmingham </td>
   <td style="text-align:right;"> 581 </td>
   <td style="text-align:right;"> 58 </td>
   <td style="text-align:right;"> 0.0998279 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Leeds </td>
   <td style="text-align:right;"> 323 </td>
   <td style="text-align:right;"> 24 </td>
   <td style="text-align:right;"> 0.0743034 </td>
  </tr>
</tbody>
</table>

### Constant-risk model


```r
model = function() {

    ## Likelihood
    for (i in 1:12) {
        y[i] ~ dbin(theta, n[i])
        res[i] <- (y[i] - n[i] * theta)/sqrt(n[i] * theta * (1 -
            theta))
        res2[i] <- res[i] * res[i]
    }

    ## prior
    theta ~ dunif(0, 1)
    X2.obs <- sum(res2[])

}


data = list(n = bristol$operations, y = bristol$deaths)

library(R2jags)
fit <- jags(data = data, model = model, parameters.to.save = c("theta",
    "res", "X2.obs"), n.chain = 2, n.iter = 5000, n.thin = 1,
    n.burn = 100, DIC = FALSE)
module glm loaded
module dic loaded
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 12
   Unobserved stochastic nodes: 1
   Total graph size: 102

Initializing model
fit.mcmc <- as.mcmc(fit)
summary(fit.mcmc)

Iterations = 101:5000
Thinning interval = 1 
Number of chains = 2 
Sample size per chain = 4900 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

           Mean       SD  Naive SE Time-series SE
res[1]   6.1499 0.329142 0.0033248      4.275e-03
res[10] -0.9222 0.246955 0.0024946      3.209e-03
res[11] -1.4817 0.384158 0.0038806      4.992e-03
res[12] -2.5181 0.257998 0.0026062      3.353e-03
res[2]   2.3392 0.236772 0.0023918      3.076e-03
res[3]   1.2884 0.245869 0.0024836      3.194e-03
res[4]   0.8796 0.246193 0.0024869      3.199e-03
res[5]   0.5859 0.246649 0.0024915      3.205e-03
res[6]   0.5830 0.251561 0.0025411      3.268e-03
res[7]  -0.6644 0.363689 0.0036738      4.726e-03
res[8]  -0.9968 0.325572 0.0032888      4.230e-03
res[9]  -0.8975 0.279907 0.0028275      3.637e-03
theta    0.1199 0.005574 0.0000563      7.253e-05
X2.obs  59.0388 2.764042 0.0279210      3.679e-02

2. Quantiles for each variable:

           2.5%     25%     50%     75%    97.5%
res[1]   5.5309  5.9250  6.1440  6.3703  6.80791
res[10] -1.3919 -1.0900 -0.9248 -0.7557 -0.43395
res[11] -2.2125 -1.7428 -1.4857 -1.2227 -0.72224
res[12] -3.0102 -2.6931 -2.5202 -2.3438 -2.00949
res[2]   1.8919  2.1778  2.3357  2.4982  2.81052
res[3]   0.8229  1.1210  1.2852  1.4537  1.77674
res[4]   0.4131  0.7120  0.8765  1.0452  1.36818
res[5]   0.1182  0.4181  0.5829  0.7519  1.07509
res[6]   0.1060  0.4118  0.5799  0.7523  1.08189
res[7]  -1.3555 -0.9116 -0.6684 -0.4194  0.05536
res[8]  -1.6159 -1.2181 -1.0002 -0.7774 -0.35289
res[9]  -1.4298 -1.0877 -0.9004 -0.7088 -0.34393
theta    0.1092  0.1161  0.1198  0.1236  0.13077
X2.obs  56.7590 57.0736 58.0523 60.0141 66.59561
```


```r
mat=as.matrix(as.mcmc(fit))
theta.avg=mean(mat[,"theta"])
boxplot(mat[,paste0("res[",1:12,"]")])
abline(h=0)
```

<img src="14_files/figure-html/unnamed-chunk-3-1.png" width="1152" />

```r

library("bayesplot")
library("ggplot2")
plot_title <- ggtitle("Posterior distributions  of residuals",
                      "with medians and 95% intervals")
mcmc_areas(mat,
           pars = paste0("res[",1:12,"]"),
           prob = 0.95) + plot_title
```

<img src="14_files/figure-html/unnamed-chunk-3-2.png" width="1152" />


### Independent-parameters model 


```r
model = function() {

    ## Likelihood
    for (i in 1:12) {
        y[i] ~ dbin(theta[i], n[i])
    }

    ## priors
    for (i in 1:12) {
        theta[i] ~ dunif(0, 1)
    }
}


data = list(n = bristol$operations, y = bristol$deaths)

fit <- jags(data = data, model = model, parameters.to.save = c("theta"),
    n.chain = 2, n.iter = 5000, n.thin = 1, n.burn = 100, DIC = FALSE)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 12
   Unobserved stochastic nodes: 12
   Total graph size: 38

Initializing model
fit.mcmc <- as.mcmc(fit)
summary(fit.mcmc)

Iterations = 101:5000
Thinning interval = 1 
Number of chains = 2 
Sample size per chain = 4900 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

             Mean      SD  Naive SE Time-series SE
theta[1]  0.28964 0.03750 0.0003788      0.0004795
theta[10] 0.10412 0.01983 0.0002004      0.0002510
theta[11] 0.10153 0.01264 0.0001277      0.0001647
theta[12] 0.07702 0.01484 0.0001499      0.0001984
theta[2]  0.19362 0.03521 0.0003557      0.0004463
theta[3]  0.15707 0.02807 0.0002836      0.0003602
theta[4]  0.14576 0.02647 0.0002674      0.0003341
theta[5]  0.13724 0.02476 0.0002501      0.0003207
theta[6]  0.13719 0.02426 0.0002451      0.0003026
theta[7]  0.11155 0.01438 0.0001452      0.0001784
theta[8]  0.10582 0.01518 0.0001533      0.0001856
theta[9]  0.10571 0.01731 0.0001749      0.0002253

2. Quantiles for each variable:

             2.5%     25%     50%     75%  97.5%
theta[1]  0.21849 0.26348 0.28889 0.31486 0.3648
theta[10] 0.06844 0.08992 0.10300 0.11692 0.1452
theta[11] 0.07819 0.09279 0.10081 0.10985 0.1277
theta[12] 0.05004 0.06676 0.07615 0.08638 0.1084
theta[2]  0.12915 0.16876 0.19118 0.21676 0.2668
theta[3]  0.10638 0.13730 0.15578 0.17537 0.2154
theta[4]  0.09786 0.12732 0.14413 0.16325 0.2013
theta[5]  0.09274 0.11992 0.13583 0.15317 0.1899
theta[6]  0.09294 0.12013 0.13596 0.15311 0.1885
theta[7]  0.08494 0.10161 0.11087 0.12091 0.1411
theta[8]  0.07796 0.09523 0.10536 0.11565 0.1369
theta[9]  0.07422 0.09363 0.10484 0.11682 0.1420
```


```r
mat=as.matrix(as.mcmc(fit))

mat.ind=mat[,paste0("theta[",1:12,"]")]

plot_title <- ggtitle("Posterior distributions of thetas",
                      "with medians and 95% intervals")
mcmc_areas(mat,
           pars = paste0("theta[",1:12,"]"),
           prob = 0.95) + plot_title
```

<img src="14_files/figure-html/unnamed-chunk-5-1.png" width="1152" />


### Hierarchical model 



```r
model = function() {

    ## Likelihood
    for (i in 1:12) {
        y[i] ~ dbin(theta[i], n[i])
        logit(theta[i]) <- logit.theta[i]
        logit.theta[i] ~ dnorm(mu, inv.sigma.squared)
    }

    ## priors
    inv.sigma.squared <- 1/pow(sigma, 2)
    sigma ~ dunif(0, 100)
    mu ~ dunif(-100, 100)
}


data = list(n = bristol$operations, y = bristol$deaths)

fit <- jags(data = data, model = model, parameters.to.save = c("theta",
    "mu", "sigma"), n.chain = 2, n.iter = 5000, n.thin = 1, n.burn = 100,
    DIC = FALSE)
Compiling model graph
   Resolving undeclared variables
   Allocating nodes
Graph information:
   Observed stochastic nodes: 12
   Unobserved stochastic nodes: 14
   Total graph size: 57

Initializing model
fit.mcmc <- as.mcmc(fit)
summary(fit.mcmc)

Iterations = 101:5000
Thinning interval = 1 
Number of chains = 2 
Sample size per chain = 4900 

1. Empirical mean and standard deviation for each variable,
   plus standard error of the mean:

             Mean      SD  Naive SE Time-series SE
mu        -1.9309 0.14106 0.0014249      0.0018997
sigma      0.4267 0.13195 0.0013329      0.0027037
theta[1]   0.2496 0.03701 0.0003738      0.0006270
theta[10]  0.1064 0.01787 0.0001805      0.0002226
theta[11]  0.1026 0.01216 0.0001229      0.0001658
theta[12]  0.0842 0.01455 0.0001469      0.0002097
theta[2]   0.1706 0.03012 0.0003043      0.0004233
theta[3]   0.1462 0.02427 0.0002452      0.0003174
theta[4]   0.1376 0.02296 0.0002319      0.0002905
theta[5]   0.1325 0.02208 0.0002230      0.0002816
theta[6]   0.1325 0.02183 0.0002205      0.0002934
theta[7]   0.1123 0.01355 0.0001368      0.0001707
theta[8]   0.1072 0.01433 0.0001447      0.0001817
theta[9]   0.1073 0.01641 0.0001658      0.0002192

2. Quantiles for each variable:

              2.5%      25%      50%     75%   97.5%
mu        -2.20675 -2.01880 -1.93144 -1.8430 -1.6404
sigma      0.22923  0.33369  0.40612  0.4949  0.7390
theta[1]   0.18085  0.22344  0.24890  0.2739  0.3248
theta[10]  0.07354  0.09376  0.10572  0.1179  0.1436
theta[11]  0.07990  0.09428  0.10233  0.1108  0.1271
theta[12]  0.05739  0.07400  0.08348  0.0935  0.1146
theta[2]   0.11831  0.14911  0.16852  0.1895  0.2361
theta[3]   0.10274  0.12908  0.14512  0.1617  0.1969
theta[4]   0.09650  0.12162  0.13623  0.1522  0.1865
theta[5]   0.09314  0.11694  0.13132  0.1469  0.1797
theta[6]   0.09328  0.11738  0.13131  0.1462  0.1791
theta[7]   0.08697  0.10293  0.11178  0.1214  0.1398
theta[8]   0.08073  0.09723  0.10653  0.1165  0.1368
theta[9]   0.07744  0.09571  0.10644  0.1181  0.1416
```



```r
mat=as.matrix(as.mcmc(fit))

mat.hier=mat[,paste0("theta[",1:12,"]")]

plot_title <- ggtitle("Posterior distributions of thetas",
                      "with medians and 95% intervals")
mcmc_areas(mat,
           pars = paste0("theta[",1:12,"]"),
           prob = 0.95) + plot_title
```

<img src="14_files/figure-html/unnamed-chunk-7-1.png" width="1152" />



```r
library(reshape2)
df1 = melt(mat.ind) %>%
    mutate(model = "independent")
df2 = melt(mat.hier) %>%
    mutate(model = "hierarchical")
df = bind_rows(df1, df2)
df$hospital = c("Bristol", "Leicester", "Leeds", "Oxford", "Guys",
    "Liverpool", "Southampton", "Great Ormond St", "Newcastle",
    "Harefield", "Birmingham", "Brompton")[as.numeric(as.factor(df$Var2))]

library(tidyverse)
library(hrbrthemes)
library(viridis)
library(ggridges)

df %>%
    ggplot(aes(y = hospital, x = value, fill = model)) + geom_density_ridges(alpha = 0.6) +
    scale_fill_viridis(discrete = TRUE) + scale_color_viridis(discrete = TRUE) +
    theme_ipsum() + theme(panel.spacing = unit(0.1, "lines"),
    strip.text.x = element_text(size = 8)) + xlab("") + ylab("Posterior Distributions") +
    geom_vline(xintercept = theta.avg, linetype = "dotted", color = "blue",
        size = 1.5)
Warning: Using `size` aesthetic for lines was deprecated in ggplot2
3.4.0.
ℹ Please use `linewidth` instead.
Picking joint bandwidth of 0.00308
```

<img src="14_files/figure-html/unnamed-chunk-8-1.png" width="672" />
Vertical line is the mean theta parameter from the constant-risk model

## Example 2: James-Stein Baseball

:::{.example #baseball name="James-Stein baseball" .lizi}
See [this vignette](https://mc-stan.org/rstanarm/articles/pooling.html) for more details of this example. The James-Stein baseball data [@Efron:1977] is analyzed using `stan_glm` in the `rstanarm` package. This includes model fits with complete pooling (constant-risk model), no pooling (independent-parameter model), and partial pooling (hierarchical model). 
:::



```r
library(rstanarm)
Loading required package: Rcpp
This is rstanarm version 2.21.3
- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!
- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.
- For execution on a local, multicore CPU with excess RAM we recommend calling
  options(mc.cores = parallel::detectCores())
data(bball1970)
bball <- bball1970

# xtable(bball)

kbl(bball) %>%
    kable_classic_2(full_width = F)
```

<table class=" lightable-classic-2" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>
 <thead>
  <tr>
   <th style="text-align:left;"> Player </th>
   <th style="text-align:right;"> AB </th>
   <th style="text-align:right;"> Hits </th>
   <th style="text-align:right;"> RemainingAB </th>
   <th style="text-align:right;"> RemainingHits </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:left;"> Clemente </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 18 </td>
   <td style="text-align:right;"> 367 </td>
   <td style="text-align:right;"> 127 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Robinson </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 17 </td>
   <td style="text-align:right;"> 426 </td>
   <td style="text-align:right;"> 127 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Howard </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 16 </td>
   <td style="text-align:right;"> 521 </td>
   <td style="text-align:right;"> 144 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Johnstone </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 15 </td>
   <td style="text-align:right;"> 275 </td>
   <td style="text-align:right;"> 61 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Berry </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 14 </td>
   <td style="text-align:right;"> 418 </td>
   <td style="text-align:right;"> 114 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Spencer </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 14 </td>
   <td style="text-align:right;"> 466 </td>
   <td style="text-align:right;"> 126 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Kessinger </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 13 </td>
   <td style="text-align:right;"> 586 </td>
   <td style="text-align:right;"> 155 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Alvarado </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 12 </td>
   <td style="text-align:right;"> 138 </td>
   <td style="text-align:right;"> 29 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Santo </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 11 </td>
   <td style="text-align:right;"> 510 </td>
   <td style="text-align:right;"> 137 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Swaboda </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 11 </td>
   <td style="text-align:right;"> 200 </td>
   <td style="text-align:right;"> 46 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Petrocelli </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 10 </td>
   <td style="text-align:right;"> 538 </td>
   <td style="text-align:right;"> 142 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Rodriguez </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 10 </td>
   <td style="text-align:right;"> 186 </td>
   <td style="text-align:right;"> 42 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Scott </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 10 </td>
   <td style="text-align:right;"> 435 </td>
   <td style="text-align:right;"> 132 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Unser </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 10 </td>
   <td style="text-align:right;"> 277 </td>
   <td style="text-align:right;"> 73 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Williams </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 10 </td>
   <td style="text-align:right;"> 591 </td>
   <td style="text-align:right;"> 195 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Campaneris </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 9 </td>
   <td style="text-align:right;"> 558 </td>
   <td style="text-align:right;"> 159 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Munson </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 8 </td>
   <td style="text-align:right;"> 408 </td>
   <td style="text-align:right;"> 129 </td>
  </tr>
  <tr>
   <td style="text-align:left;"> Alvis </td>
   <td style="text-align:right;"> 45 </td>
   <td style="text-align:right;"> 7 </td>
   <td style="text-align:right;"> 70 </td>
   <td style="text-align:right;"> 14 </td>
  </tr>
</tbody>
</table>

```r

N <- nrow(bball)
K <- bball$AB
y <- bball$Hits
K_new <- bball$RemainingAB
y_new <- bball$RemainingHits

batting_avg <- function(x) print(format(round(x, digits = 3),
    nsmall = 3), quote = FALSE)
player_avgs <- y/K  # player avgs through 45 AB
tot_avg <- sum(y)/sum(K)  # overall avg through 45 AB

cat("Player averages through 45 at-bats:\n")
Player averages through 45 at-bats:
batting_avg(player_avgs)
 [1] 0.400 0.378 0.356 0.333 0.311 0.311 0.289 0.267 0.244
[10] 0.244 0.222 0.222 0.222 0.222 0.222 0.200 0.178 0.156
cat("Overall average through 45 at-bats:\n")
Overall average through 45 at-bats:
batting_avg(tot_avg)
[1] 0.265
```

### Complete pooling


```r
SEED <- 202
wi_prior <- normal(-1, 1)  # weakly informative prior on log-odds
fit_pool <- stan_glm(cbind(Hits, AB - Hits) ~ 1, data = bball, family = binomial("logit"),
                     prior_intercept = wi_prior, seed = SEED)

SAMPLING FOR MODEL 'binomial' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 2.2e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.015257 seconds (Warm-up)
Chain 1:                0.018122 seconds (Sampling)
Chain 1:                0.033379 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'binomial' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 5e-06 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.015807 seconds (Warm-up)
Chain 2:                0.017317 seconds (Sampling)
Chain 2:                0.033124 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'binomial' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 1.1e-05 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.015199 seconds (Warm-up)
Chain 3:                0.018457 seconds (Sampling)
Chain 3:                0.033656 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'binomial' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 4e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.01666 seconds (Warm-up)
Chain 4:                0.018921 seconds (Sampling)
Chain 4:                0.035581 seconds (Total)
Chain 4: 

invlogit <- plogis  # function(x) 1/(1 + exp(-x))
summary_stats <- function(posterior) {
  x <- invlogit(posterior)  # log-odds -> probabilities
  t(apply(x, 2, quantile, probs = c(0.1, 0.5, 0.9))) 
}

pool <- summary_stats(as.matrix(fit_pool))  # as.matrix extracts the posterior draws
pool <- matrix(pool,  # replicate to give each player the same estimates
               nrow(bball), ncol(pool), byrow = TRUE, 
               dimnames = list(bball$Player, c("10%", "50%", "90%")))
batting_avg(pool)
           10%   50%   90%  
Clemente   0.246 0.265 0.286
Robinson   0.246 0.265 0.286
Howard     0.246 0.265 0.286
Johnstone  0.246 0.265 0.286
Berry      0.246 0.265 0.286
Spencer    0.246 0.265 0.286
Kessinger  0.246 0.265 0.286
Alvarado   0.246 0.265 0.286
Santo      0.246 0.265 0.286
Swaboda    0.246 0.265 0.286
Petrocelli 0.246 0.265 0.286
Rodriguez  0.246 0.265 0.286
Scott      0.246 0.265 0.286
Unser      0.246 0.265 0.286
Williams   0.246 0.265 0.286
Campaneris 0.246 0.265 0.286
Munson     0.246 0.265 0.286
Alvis      0.246 0.265 0.286

invlogit <- plogis  # function(x) 1/(1 + exp(-x))
summary_stats <- function(posterior) {
  x <- invlogit(posterior)  # log-odds -> probabilities
  t(apply(x, 2, quantile, probs = c(0.1, 0.5, 0.9)))
}

pool <- summary_stats(as.matrix(fit_pool))  # as.matrix extracts the posterior draws
pool <- matrix(pool,  # replicate to give each player the same estimates
               nrow(bball), ncol(pool), byrow = TRUE,
               dimnames = list(bball$Player, c("10%", "50%", "90%")))
batting_avg(pool)
           10%   50%   90%  
Clemente   0.246 0.265 0.286
Robinson   0.246 0.265 0.286
Howard     0.246 0.265 0.286
Johnstone  0.246 0.265 0.286
Berry      0.246 0.265 0.286
Spencer    0.246 0.265 0.286
Kessinger  0.246 0.265 0.286
Alvarado   0.246 0.265 0.286
Santo      0.246 0.265 0.286
Swaboda    0.246 0.265 0.286
Petrocelli 0.246 0.265 0.286
Rodriguez  0.246 0.265 0.286
Scott      0.246 0.265 0.286
Unser      0.246 0.265 0.286
Williams   0.246 0.265 0.286
Campaneris 0.246 0.265 0.286
Munson     0.246 0.265 0.286
Alvis      0.246 0.265 0.286
```

### No pooling


```r
fit_nopool <- update(fit_pool, formula = . ~ 0 + Player, prior = wi_prior)

SAMPLING FOR MODEL 'binomial' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 3.8e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.38 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.069053 seconds (Warm-up)
Chain 1:                0.080344 seconds (Sampling)
Chain 1:                0.149397 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'binomial' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 1.1e-05 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.067713 seconds (Warm-up)
Chain 2:                0.076083 seconds (Sampling)
Chain 2:                0.143796 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'binomial' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 8e-06 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.064684 seconds (Warm-up)
Chain 3:                0.074064 seconds (Sampling)
Chain 3:                0.138748 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'binomial' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 7e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.068065 seconds (Warm-up)
Chain 4:                0.08149 seconds (Sampling)
Chain 4:                0.149555 seconds (Total)
Chain 4: 
nopool <- summary_stats(as.matrix(fit_nopool))
rownames(nopool) <- as.character(bball$Player)
batting_avg(nopool)
            
parameters   10%   50%   90%  
  Clemente   0.297 0.386 0.484
  Robinson   0.281 0.365 0.456
  Howard     0.265 0.345 0.434
  Johnstone  0.242 0.325 0.418
  Berry      0.226 0.306 0.390
  Spencer    0.225 0.304 0.391
  Kessinger  0.207 0.284 0.371
  Alvarado   0.188 0.265 0.353
  Santo      0.175 0.246 0.328
  Swaboda    0.175 0.245 0.327
  Petrocelli 0.154 0.223 0.304
  Rodriguez  0.155 0.224 0.303
  Scott      0.159 0.223 0.303
  Unser      0.153 0.224 0.307
  Williams   0.156 0.224 0.304
  Campaneris 0.138 0.204 0.284
  Munson     0.124 0.185 0.260
  Alvis      0.107 0.164 0.241
```


### Partial pooling


```r
fit_partialpool <- stan_glmer(cbind(Hits, AB - Hits) ~ (1 | Player),
    data = bball, family = binomial("logit"), prior_intercept = wi_prior,
    seed = SEED)

SAMPLING FOR MODEL 'binomial' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 2.7e-05 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 0.119321 seconds (Warm-up)
Chain 1:                0.125259 seconds (Sampling)
Chain 1:                0.24458 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'binomial' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 9e-06 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 0.141288 seconds (Warm-up)
Chain 2:                0.106701 seconds (Sampling)
Chain 2:                0.247989 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'binomial' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 6e-06 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 0.133025 seconds (Warm-up)
Chain 3:                0.143619 seconds (Sampling)
Chain 3:                0.276644 seconds (Total)
Chain 3: 

SAMPLING FOR MODEL 'binomial' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 9e-06 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 0.116647 seconds (Warm-up)
Chain 4:                0.084232 seconds (Sampling)
Chain 4:                0.200879 seconds (Total)
Chain 4: 


# shift each player's estimate by intercept (and then drop
# intercept)
shift_draws <- function(draws) {
    sweep(draws[, -1], MARGIN = 1, STATS = draws[, 1], FUN = "+")
}
alphas <- shift_draws(as.matrix(fit_partialpool))
partialpool <- summary_stats(alphas)
partialpool <- partialpool[-nrow(partialpool), ]
rownames(partialpool) <- as.character(bball$Player)
batting_avg(partialpool)
            
parameters   10%   50%   90%  
  Clemente   0.249 0.283 0.349
  Robinson   0.246 0.281 0.341
  Howard     0.243 0.277 0.332
  Johnstone  0.239 0.274 0.324
  Berry      0.237 0.271 0.317
  Spencer    0.238 0.271 0.317
  Kessinger  0.232 0.267 0.309
  Alvarado   0.226 0.265 0.303
  Santo      0.222 0.261 0.298
  Swaboda    0.221 0.261 0.301
  Petrocelli 0.217 0.258 0.293
  Rodriguez  0.215 0.258 0.294
  Scott      0.217 0.259 0.294
  Unser      0.214 0.258 0.295
  Williams   0.215 0.258 0.295
  Campaneris 0.210 0.255 0.292
  Munson     0.204 0.252 0.287
  Alvis      0.195 0.249 0.284
```

### Observed vs estimated


```r
library(ggplot2)
models <- c("complete pooling", "no pooling", "partial pooling")
estimates <- rbind(pool, nopool, partialpool)
colnames(estimates) <- c("lb", "median", "ub")
plotdata <- data.frame(estimates, observed = rep(player_avgs,
    times = length(models)), model = rep(models, each = N), row.names = NULL)

ggplot(plotdata, aes(x = observed, y = median, ymin = lb, ymax = ub)) +
    geom_hline(yintercept = tot_avg, color = "lightpink", size = 0.75) +
    geom_abline(intercept = 0, slope = 1, color = "skyblue") +
    geom_linerange(color = "gray60", size = 0.75) + geom_point(size = 2.5,
    shape = 21, fill = "gray30", color = "white", stroke = 0.2) +
    facet_grid(. ~ model) + coord_fixed() + scale_x_continuous(breaks = c(0.2,
    0.3, 0.4)) + labs(x = "Observed Hits / AB", y = "Predicted chance of hit") +
    ggtitle("Posterior Medians and 80% Intervals")
Warning: Using `size` aesthetic for lines was deprecated in ggplot2
3.4.0.
ℹ Please use `linewidth` instead.
```

<img src="14_files/figure-html/unnamed-chunk-13-1.png" width="672" />
