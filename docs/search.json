[{"path":"index.html","id":"fundamentals-of-probability","chapter":"Class 1 Fundamentals of Probability","heading":"Class 1 Fundamentals of Probability","text":"","code":""},{"path":"index.html","id":"coins-urns-and-bags","chapter":"Class 1 Fundamentals of Probability","heading":"1.1 Coins, Urns, and Bags","text":"Example 1.1  Consider 100 flips fair coin.probability observing exactly 50 heads 100 flips fair coin?probability observing exactly 50 heads 100 flips fair coin?probability observing 50 heads?probability observing 50 heads?many heads extreme sense less 5% chance observing many heads ?many heads extreme sense less 5% chance observing many heads ?Exercise 1.1  (marbles) Suppose ’s bag containing 50 marbles marble either red yellow.Five marbles randomly selected replacement one found yellow. probability marbles bag yellow?Five marbles randomly selected replacement one found yellow. probability marbles bag yellow?Five marbles randomly selected without replacement found yellow. probability marbles bag yellow?Five marbles randomly selected without replacement found yellow. probability marbles bag yellow?Example 1.2  (gemstones) Suppose \\(n\\) bags labeled \\(1,\\ldots,n\\) bag \\(\\) containing \\(\\) rubies \\(n-\\) diamonds. Suppose bag \\(\\) selected probability directly proportional \\(\\), random gemstone selected bag. probability diamond? Provide theoretical calculation simulated approximation.1.2","code":"# (a)\ndbinom(50,100,prob=1/2)\n[1] 0.07958924\n# (b)\nsum(dbinom(50:100,100,prob=1/2))\n[1] 0.5397946\n1-pbinom(49,100,prob=1/2)\n[1] 0.5397946\npbinom(49,100,prob=1/2,lower.tail=FALSE)\n[1] 0.5397946\n# (c)\nqbinom(.95,100,prob=1/2)\n[1] 58\nqbinom(.05,100,prob=1/2,lower.tail=FALSE)\n[1] 58\n1-pbinom(57,100,prob=1/2)\n[1] 0.06660531\n1-pbinom(58,100,prob=1/2)\n[1] 0.04431304n=13\nPr_given_B=(1:n)/n\nPr_of_B=(1:n)/sum(1:n)\nsum(Pr_given_B * Pr_of_B)\n[1] 0.6923077\n(2*n+1)/(3*n)\n[1] 0.6923077\n\nR=10^6 # number of random draws\nB=1:n\nx1=sample(B,size=R,replace=T,prob=B)\nx2=sapply(x1,function(x){rbinom(1,1,prob=x/n)})\nmean(x2)  \n[1] 0.692364"},{"path":"bayes-theorem.html","id":"bayes-theorem","chapter":"Class 2 Bayes’ Theorem","heading":"Class 2 Bayes’ Theorem","text":"","code":""},{"path":"bayes-theorem.html","id":"examples","chapter":"Class 2 Bayes’ Theorem","heading":"2.1 Examples","text":"","code":""},{"path":"software.html","id":"software","chapter":"Class 3 Software","heading":"Class 3 Software","text":"","code":""},{"path":"software.html","id":"jags","chapter":"Class 3 Software","heading":"3.1 JAGS","text":"","code":"library(R2jags)\n\nmymodel= function(){\nfor ( i in 1:Ntotal ) {\n    y[i] ~ dbern( theta )\n  }\n  theta ~ dbeta( 1 , 1 )\n}\n\nn <- 50\ntheta.true <- .3\ny.sim <- rbinom(n, 1, theta.true)\n\nmydata = list(    \n  y = y.sim ,\n  Ntotal = n \n)\n\n\n\nfit <- jags(data=mydata, model=mymodel, parameters.to.save = c(\"theta\"), n.chain=2, n.iter=200, n.thin=1, n.burn=100)\nmodule glm loaded\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 50\n   Unobserved stochastic nodes: 1\n   Total graph size: 53\n\nInitializing model\nfit\nInference for Bugs model at \"/var/folders/fy/_0t49sys0713k84msqwk44fc0000gp/T//RtmpXLbqSR/model4cee6db30705.txt\", fit using jags,\n 2 chains, each with 200 iterations (first 100 discarded)\n n.sims = 200 iterations saved\n         mu.vect sd.vect   2.5%    25%    50%    75%  97.5%\ntheta      0.366   0.068  0.252  0.323  0.360  0.407  0.510\ndeviance  66.337   1.585 65.343 65.441 65.712 66.511 71.504\n          Rhat n.eff\ntheta    0.998   200\ndeviance 1.021   200\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 1.3 and DIC = 67.6\nDIC is an estimate of expected predictive error (lower deviance is better).library(R2jags)\n\nmodel= function(){\nM ~ dcat(p1)\n# sampling distribution is uniform over first N integers\n# use step function to change p1[j] to 0 for j>N\nfor (j in 1:1000) {\np1[j] <- step(N - j + 0.01)/N\n}\nN ~ dcat(p2)\nfor (j in 1:1000) {\nrecip[j] <- 1/j\np2[j] <- recip[j]/sum.recip\n}\nsum.recip <- sum(recip)\n}\ndata=list(\"M\"=100)\nfit <- jags(data=data, model=model,parameters.to.save=c(\"N\",\"M\"), n.chain=2, n.iter=100, n.thin=1, n.burn=0, DIC=FALSE)\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1\n   Unobserved stochastic nodes: 1\n   Total graph size: 7007\n\nInitializing model\nfit\nInference for Bugs model at \"/var/folders/fy/_0t49sys0713k84msqwk44fc0000gp/T//RtmpXLbqSR/model4cee11bcb869.txt\", fit using jags,\n 2 chains, each with 100 iterations (first 0 discarded)\n n.sims = 200 iterations saved\n  mu.vect sd.vect 2.5% 25%   50%    75%   97.5%  Rhat n.eff\nM 100.000   0.000  100 100 100.0 100.00 100.000 1.000     1\nN 264.665 196.366  101 127 184.5 321.75 836.275 0.997   200\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).library(rjags)\nmodel_string <- \"\nmodel {\nM ~ dcat(p1)\nfor (j in 1:1000) {\np1[j] <- (pow(j*step(N - j + 0.01)/N,2)-pow((j-1)*step(N - j  + 0.01)/N,2))\n}\nN ~ dcat(p2[])\nfor (j in 1:1000) {\nrecip[j] <- 1/j\np2[j] <- recip[j]/sum.recip\n}\nsum.recip <- sum(recip)\n}\"\n\ndata=list(\"M\"=50)\njmodel <- jags.model(textConnection(model_string), data = data,inits=list(N=100),n.chains = 2, n.adapt= 100,quiet=TRUE)\nupdate(jmodel, 10)\nmcmc_samples <- coda.samples(jmodel, variable.names=c(\"N\"), n.iter=100)\nsummary(mcmc_samples)\n\nIterations = 11:110\nThinning interval = 1 \nNumber of chains = 2 \nSample size per chain = 100 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n       103.990         89.148          6.304          6.318 \n\n2. Quantiles for each variable:\n\n  2.5%    25%    50%    75%  97.5% \n 51.00  57.75  77.50 110.25 364.58 "}]
