[{"path":"index.html","id":"class-1","chapter":"1 Class 1","heading":"1 Class 1","text":"","code":""},{"path":"index.html","id":"probability-fundamentals","chapter":"1 Class 1","heading":"1.1 Probability Fundamentals","text":"Example 1.1  (binomial flips) Consider 100 flips fair coin.probability observing exactly 50 heads 100 flips fair coin?probability observing exactly 50 heads 100 flips fair coin?probability observing 50 heads?probability observing 50 heads?many heads extreme sense less 5% chance observing many heads ?many heads extreme sense less 5% chance observing many heads ?Exercise 1.1  (marbles) Suppose ’s bag containing 50 marbles marble either red yellow.Five marbles randomly selected replacement one found yellow. probability marbles bag yellow?Five marbles randomly selected replacement one found yellow. probability marbles bag yellow?Five marbles randomly selected without replacement found yellow. probability marbles bag yellow?Five marbles randomly selected without replacement found yellow. probability marbles bag yellow?","code":"# (a)\ndbinom(50, 100, prob = 1/2)\n[1] 0.07958924\n# (b)\nsum(dbinom(50:100, 100, prob = 1/2))\n[1] 0.5397946\n1 - pbinom(49, 100, prob = 1/2)\n[1] 0.5397946\npbinom(49, 100, prob = 1/2, lower.tail = FALSE)\n[1] 0.5397946\n# (c)\nqbinom(0.95, 100, prob = 1/2)\n[1] 58\nqbinom(0.05, 100, prob = 1/2, lower.tail = FALSE)\n[1] 58\n1 - pbinom(57, 100, prob = 1/2)\n[1] 0.06660531\n1 - pbinom(58, 100, prob = 1/2)\n[1] 0.04431304"},{"path":"index.html","id":"monte-carlo","chapter":"1 Class 1","heading":"1.2 Monte Carlo","text":"Example 1.2  (gemstones) Suppose \\(n\\) bags labeled \\(1,\\ldots,n\\) bag \\(\\) containing \\(\\) rubies \\(n-\\) diamonds. Suppose bag \\(\\) selected probability directly proportional \\(\\), random gemstone selected bag. probability diamond? Provide theoretical calculation simulated approximation.Example 1.3  (repairs) Suppose costs repair gamma distribution mean $100 standard deviation $50. many items able repair $1000?Exercise 1.2  (ICER) Suppose patient heart failure survival time exponential mean \\(\\theta_N\\) years. Suppose heart transplant \\(\\theta_T\\) operative survival rate, , survive, survival, \\(s_P\\), follows exponential distribution mean \\(\\theta_P\\). Assume operation costs \\(D_{\\text{operation}}\\) dollars post-operative year medical costs immunosuppressants prescriptions amount \\(D_\\text{annual}\\) dollars.literature questions , use Milliman Research Report 2020 U.S. organ tissue transplants possible. Parts (g) (h) relate incremental cost-effectiveness ratio. part (g) requires Monte Carlo simulation.Identify reasonable value \\(\\theta_N\\) literature.Identify reasonable value \\(\\theta_N\\) literature.Calculated monthly mortality based value \\(\\theta_N\\) determined ().Calculated monthly mortality based value \\(\\theta_N\\) determined ().Identify reasonable value \\(\\theta_T\\) literature.Identify reasonable value \\(\\theta_T\\) literature.Identify reasonable value \\(\\theta_P\\) literature.Identify reasonable value \\(\\theta_P\\) literature.Identify reasonable value \\(D_{\\text{operation}}\\) literature.Identify reasonable value \\(D_{\\text{operation}}\\) literature.Identify reasonable value \\(D_{\\text{annual}}\\) literature.Identify reasonable value \\(D_{\\text{annual}}\\) literature.Based values found , estimate expected additional cost per year life gained transplant?Based values found , estimate expected additional cost per year life gained transplant?Calculate ratio expected additional cost expected additional benefit.Calculate ratio expected additional cost expected additional benefit.","code":"n = 13\nPr_given_B = (1:n)/n\nPr_of_B = (1:n)/sum(1:n)\nsum(Pr_given_B * Pr_of_B)\n[1] 0.6923077\n(2 * n + 1)/(3 * n)\n[1] 0.6923077\n\nR = 10^6  # number of random draws\nB = 1:n\nx1 = sample(B, size = R, replace = T, prob = B)\nx2 = sapply(x1, function(x) {\n    rbinom(1, 1, prob = x/n)\n})\nmean(x2)\n[1] 0.692506R = 10^4  # number of random draws\nX.mean = 100\nX.var = 50^2\ns = X.var/X.mean\na = X.mean/s\na * s  #mean\n[1] 100\na * s^2  #variance\n[1] 2500\n\nX = rgamma(20, shape = a, scale = s)\ncumsum(X)\n [1]   69.91186  207.28872  237.96179  294.58408  325.60142\n [6]  390.36803  568.03599  628.85510  719.53580  854.38467\n[11] 1012.79237 1088.89943 1280.43185 1335.85219 1396.11262\n[16] 1493.13453 1557.70749 1708.38315 1743.72706 1853.91936\nwhich(cumsum(X) <= 1000)\n [1]  1  2  3  4  5  6  7  8  9 10\nmax(which(cumsum(X) <= 1000))\n[1] 10\n\nres = rep(as.integer(NA), R)\nfor (i in 1:R) {\n    X = rgamma(20, shape = a, scale = s)\n    res[i] = max(which(cumsum(X) <= 1000))\n}\nmax(res)\n[1] 17\nmean(res)\n[1] 9.6326\nsd(res)\n[1] 1.607133\n\nlibrary(ggplot2)\ndf = data.frame(number = as.factor(res))\nggplot(df, aes(x = number)) + geom_bar()"},{"path":"class-2.html","id":"class-2","chapter":"2 Class 2","heading":"2 Class 2","text":"","code":""},{"path":"class-2.html","id":"diagnostic-testing","chapter":"2 Class 2","heading":"2.1 Diagnostic Testing","text":"Example 2.1  (pregnancy testing) Human Chorionic Gonadotropin (hCG) hormone dramatically increases pregnancy. Levels hormone may also high individuals cancers.Levels hCG can first detected blood test 11 days conception 12-14 days conception urine test, peak first 8-11 weeks pregnancy decline level remainder pregnancy.Non-pregnant women hCG levels < 5 IU/L hCG values > 25 IU/L indicate pregnancy. Levels 5 25 IU/L often indicate early pregnancy, results need interpreted cautiously false positive results can occur range.example simulate hCG data hypothetical population non-pregnant pregnant women early pregnancy, use data study various statistics graphics used classifier assessment.ROC graphic AUC computation using ROCR package1. First “early” pregnancy simulation (mean 20 truncated normal distribution).easily identify suitable threshold graphic, redo graphic biomarker values displayed log scale.Identifying “optimal” cutoff depends costs associated false positives false negatives. can read introduction ROCR. say worse case – false positive pregnancy test false negative pregnancy test? , ’d rather false negative pleasantly surprised later, others might --pleasant surprise. reality, course, “inconclusive” option, doesn’t fit well rigid statistical classifications gives rise various confusion matrix statistics.\nfollowing calculation maximizes Youden’s index2 equal cost false positives false negatives.see cutoff corresponds minimizing sum probability false positive probability false negative demonstrated following code.graph false positives false negatives cutoff original graphic.’s calculation cost associated false positives false negatives, specifically 4 times cost false positive compared false negative.display cutoff graphic showing decreased probability false positive.Next “slightly later” pregnancy simulation (mean 30 truncated normal distribution).compute “optimal” cutoff greater cost false positive.Example 2.2  (PPV prevalence) user-specified values sensitivity specificity, graph positive predictive value range prevalence values.","code":"\nlibrary(truncnorm)\nmygraph = function(mymean) {\n    x <- seq(0, 45, 0.001)\n    y1 <- dlnorm(x, mean = 1.25, sd = 1.5)\n    y2 <- dtruncnorm(x, a = 0, mean = mymean, sd = 5)\n    plot(x, y1, type = \"n\", xlab = \"hcg (mIU/ml)\", cex.lab = 1.5,\n        cex.axis = 1.5, lwd = 4, ylab = \"\")\n\n    ind1 = (x >= 5 & x <= 25)\n    polygon(c(5, x[ind1], 25), c(0, y1[ind1], 0), col = \"red\",\n        border = NA)\n    polygon(c(5, x[ind1], 25), c(0, y2[ind1], 0), col = \"red\",\n        border = NA)\n\n    ind2 = (x < 5)\n    polygon(c(0, x[ind2], 5), c(0, y1[ind2], 0), col = \"blue\",\n        border = NA)\n\n    ind3 = (x > 25)\n    polygon(c(25, x[ind3], 45), c(0, y1[ind3], 0), col = \"magenta\",\n        border = NA)\n    polygon(c(25, x[ind3], 45), c(0, y2[ind3], 0), col = \"magenta\",\n        border = NA)\n\n    points(x, y1, type = \"l\", lwd = 4)\n    points(x, y2, type = \"l\", lwd = 4)\n\n    abline(v = 5, lwd = 3, lty = 3)\n    abline(v = 25, lwd = 3, lty = 3)\n    axis(1, at = 5, cex.axis = 1.5)\n    axis(1, at = 25, cex.axis = 1.5)\n}\n\nmygraph(20)  # representing very early on in the pregnancy\nmygraph(30)  # slightly later in the pregnancy\nlibrary(ROCR)\nn = 10^5\ny1 <- rlnorm(n, mean = 1.25, sd = 1.5)\ny2 <- rtruncnorm(n, a = 0, mean = 20, sd = 5)\ny3 <- rtruncnorm(n, a = 0, mean = 30, sd = 5)\ndf = data.frame(hcg = c(y1, y2), preg = c(rep(0, n), rep(1, n)))\npred1 = prediction(df$hcg, df$preg)\nperf1 <- performance(pred1, \"tpr\", \"fpr\")\nauc.perf1 = performance(pred1, measure = \"auc\")@y.values[[1]]\nplot(perf1, colorize = TRUE, downsampling = 10^3)\ntext(1, 0.1, paste0(\"AUC=\", signif(auc.perf1, 3)), adj = 1)\ndf = data.frame(hcg.log = log(c(y1, y2)), preg = c(rep(\"no\",\n    n), rep(\"yes\", n)))\npred2 = prediction(df$hcg.log, df$preg)\nperf2 <- performance(pred2, \"tpr\", \"fpr\")\nauc.perf2 = performance(pred2, measure = \"auc\")@y.values[[1]]\nplot(perf2, colorize = TRUE, downsampling = 10^3)\ntext(1, 0.1, paste0(\"AUC=\", signif(auc.perf2, 3)), adj = 1)cost.perf2 = performance(pred2, \"cost\", cost.fp = 1, cost.fn = 1)\npred2@cutoffs[[1]][which.min(cost.perf2@y.values[[1]])]\n[1] 2.37613\nmycutoff = exp(pred2@cutoffs[[1]][which.min(cost.perf2@y.values[[1]])])\nmycutoff\n[1] 10.76317J = function(x) {\n    mean(y1 > x) + mean(y2 < x)\n}\nJ.vec = Vectorize(J, \"x\")\nx = seq(0, 20, 0.1)\nx[which.min(J.vec(x))]\n[1] 11.2\nmycutoff\n[1] 10.76317\nx <- seq(0, 45, 0.001)\ny1.plot <- dlnorm(x, mean = 1.25, sd = 1.5)\ny2.plot <- dtruncnorm(x, a = 0, mean = 20, sd = 5)\n\nplot(x, y1.plot, type = \"n\", xlab = \"hcg (mIU/ml)\", cex.lab = 1.5,\n    cex.axis = 1.5, lwd = 4, ylab = \"\")\n\nind1 = (x >= mycutoff)\npolygon(c(mycutoff, x[ind1], 45), c(0, y2.plot[ind1], 0), col = \"magenta\",\n    border = NA)\npolygon(c(mycutoff, x[ind1], 45), c(0, y1.plot[ind1], 0), col = \"red\",\n    border = NA)\nind2 = (x <= mycutoff)\npolygon(c(0, x[ind2], mycutoff), c(0, y1.plot[ind2], 0), col = \"blue\",\n    border = NA)\npolygon(c(0, x[ind2], mycutoff), c(0, y2.plot[ind2], 0), col = \"orange\",\n    border = NA)\n\npoints(x, y1.plot, type = \"l\", lwd = 4)\npoints(x, y2.plot, type = \"l\", lwd = 4)\n\nabline(v = mycutoff, lwd = 2, lty = 2)\nabline(v = 5, lwd = 1, lty = 3)\nabline(v = 25, lwd = 1, lty = 3)\naxis(1, at = 5, cex.axis = 1.5)\naxis(1, at = 25, cex.axis = 1.5)cost.perf3 = performance(pred2, \"cost\", cost.fp = 4, cost.fn = 1)\npred2@cutoffs[[1]][which.min(cost.perf3@y.values[[1]])]\n[1] 2.681491\nexp(pred2@cutoffs[[1]][which.min(cost.perf3@y.values[[1]])])\n[1] 14.60685\nx <- seq(0, 45, 0.001)\ny1.plot <- dlnorm(x, mean = 1.25, sd = 1.5)\ny2.plot <- dtruncnorm(x, a = 0, mean = 20, sd = 5)\nmycutoff = exp(pred2@cutoffs[[1]][which.min(cost.perf3@y.values[[1]])])\n\nplot(x, y1.plot, type = \"n\", xlab = \"hcg (mIU/ml)\", cex.lab = 1.5,\n    cex.axis = 1.5, lwd = 4, ylab = \"\")\n\nind1 = (x >= mycutoff)\npolygon(c(mycutoff, x[ind1], 45), c(0, y2.plot[ind1], 0), col = \"magenta\",\n    border = NA)\npolygon(c(mycutoff, x[ind1], 45), c(0, y1.plot[ind1], 0), col = \"red\",\n    border = NA)\nind2 = (x <= mycutoff)\npolygon(c(0, x[ind2], mycutoff), c(0, y1.plot[ind2], 0), col = \"blue\",\n    border = NA)\npolygon(c(0, x[ind2], mycutoff), c(0, y2.plot[ind2], 0), col = \"orange\",\n    border = NA)\n\npoints(x, y1.plot, type = \"l\", lwd = 4)\npoints(x, y2.plot, type = \"l\", lwd = 4)\n\nabline(v = mycutoff, lwd = 2, lty = 2)\nabline(v = 5, lwd = 1, lty = 3)\nabline(v = 25, lwd = 1, lty = 3)\naxis(1, at = 5, cex.axis = 1.5)\naxis(1, at = 25, cex.axis = 1.5)\ndf = data.frame(hcg.log = log(c(y1, y3)), preg = c(rep(\"no\",\n    n), rep(\"yes\", n)))\npred3 = prediction(df$hcg.log, df$preg)\nperf3 <- performance(pred3, \"tpr\", \"fpr\")\nauc.perf3 = performance(pred3, measure = \"auc\")@y.values[[1]]\nplot(perf3, colorize = TRUE)\ntext(1, 0.1, paste0(\"AUC=\", signif(auc.perf3, 3)), adj = 1)cost.perf4 = performance(pred3, \"cost\", cost.fp = 4, cost.fn = 1)\npred3@cutoffs[[1]][which.min(cost.perf4@y.values[[1]])]\n[1] 3.090492\nexp(pred3@cutoffs[[1]][which.min(cost.perf4@y.values[[1]])])\n[1] 21.9879\nsensitivity = 0.98\nspecificity = 0.999\nprevalence = 10^(-5:-1)\nppv = sensitivity * prevalence/(sensitivity * prevalence + (1 -\n    specificity) * (1 - prevalence))\nplot(log(prevalence, 10), ppv, ylab = \"Positive Predictive Value\",\n    xlab = expression(\"Log\"[10] * \"(Prevalence)\"), pch = 16)"},{"path":"class-3.html","id":"class-3","chapter":"3 Class 3","heading":"3 Class 3","text":"","code":""},{"path":"class-3.html","id":"bayes-theorem-practice","chapter":"3 Class 3","heading":"3.1 Bayes’ Theorem Practice","text":"Example 3.1  (single roll) Suppose friend 3 dice. One 4 sides, one 6 sides, one 8 sides. draws one die random, rolls one time without showing , reports result rolled 2. calculate probability die 4 sided die, probability 6 sided die, probability 8 sided die?Example 3.2  (multiple rolls) Now suppose 6 possible dice, one 4 sides, one 6 sides, one 8 sides, one 10 sides, one 12 sides, one 20 sides. randomly selecting die, die rolled n times outcomes reported. Numerically calculate probabilities number sides randomly selected die.Exercise 3.1  (erroneous dice) Suppose 6 possible dice: one 4 sides, one 6 sides, one 8 sides, one 10 sides, one 12 sides, one 20 sides. randomly selecting die, die rolled n times outcomes reported possible errors reported rolls. assume ’s error given roll, random number 1 20. Incorporate error rate likelihood uniform prior. Given vector rolls, rolls , calculate graph posterior probabilities error rate number sides.","code":"# prior probabilities: P(n=4), P(n=6), P(n=8)\nprob.prior = rep(1/3, 3)\n\n# likelihood probabilities: P(roll=2 | n=4), etc.\nprob.likelihood = c(1/4, 1/6, 1/8)\n\n# posterior probabililities P(n=4 | roll=2)\nprob.posterior = prob.prior * prob.likelihood/sum(prob.prior *\n    prob.likelihood)\n\nprob.posterior\n[1] 0.4615385 0.3076923 0.2307692\n\nc(6, 4, 3)/13\n[1] 0.4615385 0.3076923 0.2307692dice = c(4, 6, 8, 10, 12, 20)\n\nnum.sides = sample(dice, 1)\n\nnum.rolls = 6\n\nrolls = sample(1:num.sides, num.rolls, replace = TRUE)\n\nrolls\n[1] 8 9 7 9 7 9\n\nprior = rep(1/6, 6)\n\nlikelihood = (1/dice)^num.rolls * (max(rolls) <= dice)\n\nmarginal = sum(prior * likelihood)\n\nposterior = (prior * likelihood)/marginal\n\nposterior\n[1] 0.00000000 0.00000000 0.00000000 0.74045390 0.24797651\n[6] 0.01156959\n\nbarplot(posterior, names = dice, main = paste0(\"Actual number of sides: \",\n    num.sides))dice=c(4,6,8,10,12,20)\nnum.sides = sample(dice,1)\nn = 10\nerror.param=runif(1)\nerrors.ind=rbinom(n,1,prob=error.param)\nerrors.values = sample(1:20,sum(errors.ind),replace=T)\n\nrolls = sample(1:num.sides,n,replace=TRUE)\nrolls[which(errors.ind==1)]=errors.values\nrolls\n [1] 7 4 7 3 2 4 7 8 6 7"},{"path":"class-4.html","id":"class-4","chapter":"4 Class 4","heading":"4 Class 4","text":"","code":""},{"path":"class-4.html","id":"beta-binomial","chapter":"4 Class 4","heading":"4.1 Beta-binomial","text":"Exercise 4.1  (Alice Bob simulation) Verify answer Alice Bob homework exercise Monte Carlo simulation.","code":""},{"path":"class-5.html","id":"class-5","chapter":"5 Class 5","heading":"5 Class 5","text":"","code":""},{"path":"class-5.html","id":"covid-19-vaccine-trials","chapter":"5 Class 5","heading":"5.1 COVID-19 Vaccine Trials","text":"Exercise 5.1  (Vaccine Efficacy) Reproduce plot similar Figure 3 in3 provide table ten left right endpoints graphic.","code":""},{"path":"to-be-incorporated.html","id":"to-be-incorporated","chapter":"6 To Be Incorporated","heading":"6 To Be Incorporated","text":"","code":""},{"path":"to-be-incorporated.html","id":"jags","chapter":"6 To Be Incorporated","heading":"6.1 JAGS","text":"","code":"library(R2jags)\nLoading required package: rjags\nLoading required package: coda\nLinked to JAGS 4.3.0\nLoaded modules: basemod,bugs\n\nAttaching package: 'R2jags'\nThe following object is masked from 'package:coda':\n\n    traceplot\n\nmymodel= function(){\nfor ( i in 1:Ntotal ) {\n    y[i] ~ dbern( theta )\n  }\n  theta ~ dbeta( 1 , 1 )\n}\n\nn <- 50\ntheta.true <- .3\ny.sim <- rbinom(n, 1, theta.true)\n\nmydata = list(    \n  y = y.sim ,\n  Ntotal = n \n)\n\n\n\nfit <- jags(data=mydata, model=mymodel, parameters.to.save = c(\"theta\"), n.chain=2, n.iter=200, n.thin=1, n.burn=100)\nmodule glm loaded\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 50\n   Unobserved stochastic nodes: 1\n   Total graph size: 53\n\nInitializing model\nfit\nInference for Bugs model at \"/var/folders/fy/_0t49sys0713k84msqwk44fc0000gp/T//RtmphaFY5e/model12f5f37eb986b.txt\", fit using jags,\n 2 chains, each with 200 iterations (first 100 discarded)\n n.sims = 200 iterations saved\n         mu.vect sd.vect   2.5%    25%    50%    75%  97.5%\ntheta      0.325   0.062  0.201  0.284  0.329  0.369  0.433\ndeviance  63.604   1.244 62.688 62.799 63.201 63.903 66.765\n          Rhat n.eff\ntheta    1.045    82\ndeviance 1.132    47\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 0.8 and DIC = 64.4\nDIC is an estimate of expected predictive error (lower deviance is better).library(R2jags)\n\nmodel= function(){\nM ~ dcat(p1)\n# sampling distribution is uniform over first N integers\n# use step function to change p1[j] to 0 for j>N\nfor (j in 1:1000) {\np1[j] <- step(N - j + 0.01)/N\n}\nN ~ dcat(p2)\nfor (j in 1:1000) {\nrecip[j] <- 1/j\np2[j] <- recip[j]/sum.recip\n}\nsum.recip <- sum(recip)\n}\ndata=list(\"M\"=100)\nfit <- jags(data=data, model=model,parameters.to.save=c(\"N\",\"M\"), n.chain=2, n.iter=100, n.thin=1, n.burn=0, DIC=FALSE)\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1\n   Unobserved stochastic nodes: 1\n   Total graph size: 7007\n\nInitializing model\nfit\nInference for Bugs model at \"/var/folders/fy/_0t49sys0713k84msqwk44fc0000gp/T//RtmphaFY5e/model12f5f261fc56b.txt\", fit using jags,\n 2 chains, each with 100 iterations (first 0 discarded)\n n.sims = 200 iterations saved\n  mu.vect sd.vect    2.5%    25% 50%    75%   97.5%  Rhat\nM  100.00   0.000 100.000 100.00 100 100.00 100.000 1.000\nN  259.14 199.757 102.975 130.75 182 292.25 856.125 0.996\n  n.eff\nM     1\nN   200\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).library(rjags)\nmodel_string <- \"\nmodel {\nM ~ dcat(p1)\nfor (j in 1:1000) {\np1[j] <- (pow(j*step(N - j + 0.01)/N,2)-pow((j-1)*step(N - j  + 0.01)/N,2))\n}\nN ~ dcat(p2[])\nfor (j in 1:1000) {\nrecip[j] <- 1/j\np2[j] <- recip[j]/sum.recip\n}\nsum.recip <- sum(recip)\n}\"\n\ndata=list(\"M\"=50)\njmodel <- jags.model(textConnection(model_string), data = data,inits=list(N=100),n.chains = 2, n.adapt= 100,quiet=TRUE)\nupdate(jmodel, 10)\nmcmc_samples <- coda.samples(jmodel, variable.names=c(\"N\"), n.iter=100)\nsummary(mcmc_samples)\n\nIterations = 11:110\nThinning interval = 1 \nNumber of chains = 2 \nSample size per chain = 100 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n       103.310         86.080          6.087          6.099 \n\n2. Quantiles for each variable:\n\n  2.5%    25%    50%    75%  97.5% \n 50.00  58.75  71.50 111.50 325.80 "},{"path":"references.html","id":"references","chapter":"7 References","heading":"7 References","text":"","code":""}]
