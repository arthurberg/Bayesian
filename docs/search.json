[{"path":"index.html","id":"section","chapter":"Class 1 ","heading":"Class 1 ","text":"","code":""},{"path":"index.html","id":"probability-fundamentals","chapter":"Class 1 ","heading":"1.1 Probability Fundamentals","text":"Example 1.1  (binomial flips) Consider 100 flips fair coin.probability observing exactly 50 heads 100 flips fair coin?probability observing exactly 50 heads 100 flips fair coin?probability observing 50 heads?probability observing 50 heads?many heads extreme sense less 5% chance observing many heads ?many heads extreme sense less 5% chance observing many heads ?Exercise 1.1  (marbles) Suppose ’s bag containing 50 marbles marble either red yellow.Five marbles randomly selected replacement one found yellow. probability marbles bag yellow?Five marbles randomly selected replacement one found yellow. probability marbles bag yellow?Five marbles randomly selected without replacement found yellow. probability marbles bag yellow?Five marbles randomly selected without replacement found yellow. probability marbles bag yellow?","code":"# (a)\ndbinom(50,100,prob=1/2)\n[1] 0.07958924\n# (b)\nsum(dbinom(50:100,100,prob=1/2))\n[1] 0.5397946\n1-pbinom(49,100,prob=1/2)\n[1] 0.5397946\npbinom(49,100,prob=1/2,lower.tail=FALSE)\n[1] 0.5397946\n# (c)\nqbinom(.95,100,prob=1/2)\n[1] 58\nqbinom(.05,100,prob=1/2,lower.tail=FALSE)\n[1] 58\n1-pbinom(57,100,prob=1/2)\n[1] 0.06660531\n1-pbinom(58,100,prob=1/2)\n[1] 0.04431304"},{"path":"index.html","id":"monte-carlo","chapter":"Class 1 ","heading":"1.2 Monte Carlo","text":"Example 1.2  (gemstones) Suppose \\(n\\) bags labeled \\(1,\\ldots,n\\) bag \\(\\) containing \\(\\) rubies \\(n-\\) diamonds. Suppose bag \\(\\) selected probability directly proportional \\(\\), random gemstone selected bag. probability diamond? Provide theoretical calculation simulated approximation.Example 1.3  (repairs) Suppose costs repair gamma distribution mean $100 standard deviation $50. many items able repair $1000?Exercise 1.2  (ICER) Suppose patient heart failure survival time \\(s_N\\), assumed exponential mean \\(\\theta_N\\) years. Suppose heart transplant \\(\\theta_T\\) operative survival rate, , survive, survival, \\(s_P\\), follows exponential distribution mean \\(\\theta_P\\). Assume operation costs \\(D_{\\text{operation}}\\) dollars post-operative year medical costs immunosuppressants prescriptions amount \\(D_\\text{annual}\\) dollars.literature questions , use possible.Identify reasonable value \\(\\theta_N\\) literature.Identify reasonable value \\(\\theta_N\\) literature.Calculated monthly mortality based value \\(\\theta_N\\) determined ().Calculated monthly mortality based value \\(\\theta_N\\) determined ().Identify reasonable value \\(\\theta_T\\) literature.Identify reasonable value \\(\\theta_T\\) literature.Identify reasonable value \\(\\theta_P\\) literature.Identify reasonable value \\(\\theta_P\\) literature.Identify reasonable value \\(D_{\\text{operation}}\\) literature.Identify reasonable value \\(D_{\\text{operation}}\\) literature.Identify reasonable value \\(D_{\\text{annual}}\\) literature.Identify reasonable value \\(D_{\\text{annual}}\\) literature.Based values found , estimate expected additional cost per year life gained transplant?Based values found , estimate expected additional cost per year life gained transplant?Calculate ratio expected additional cost expected additional benefit.Calculate ratio expected additional cost expected additional benefit.","code":"n=13\nPr_given_B=(1:n)/n\nPr_of_B=(1:n)/sum(1:n)\nsum(Pr_given_B * Pr_of_B)\n[1] 0.6923077\n(2*n+1)/(3*n)\n[1] 0.6923077\n\nR=10^6 # number of random draws\nB=1:n\nx1=sample(B,size=R,replace=T,prob=B)\nx2=sapply(x1,function(x){rbinom(1,1,prob=x/n)})\nmean(x2)  \n[1] 0.692364R=10^4 # number of random draws\nX.mean = 100\nX.var = 50^2\ns=X.var/X.mean\na=X.mean/s\na*s #mean\n[1] 100\na*s^2 #variance\n[1] 2500\n\nX = rgamma(20, shape=a, scale = s)\ncumsum(X)\n [1]  152.2243  300.7777  352.8082  444.5944  501.6780\n [6]  594.7768  683.3199  768.5335  819.8692  907.9583\n[11] 1066.9384 1206.0384 1321.3003 1382.9581 1448.2369\n[16] 1636.1918 1654.1601 1835.9000 1903.0882 1958.0845\nwhich(cumsum(X)<=1000)\n [1]  1  2  3  4  5  6  7  8  9 10\nmax(which(cumsum(X)<=1000))\n[1] 10\n\nres=rep(as.integer(NA),R)\nfor(i in 1:R){\n  X = rgamma(20, shape=a, scale = s)\n  res[i]=max(which(cumsum(X)<=1000))\n}\nmax(res)\n[1] 16\nmean(res)\n[1] 9.5898\nsd(res)\n[1] 1.620493\n\nlibrary(ggplot2)\ndf=data.frame(number=as.factor(res))\nggplot(df, aes(x=number)) + geom_bar()"},{"path":"bayes-theorem.html","id":"bayes-theorem","chapter":"Class 2 Bayes’ Theorem","heading":"Class 2 Bayes’ Theorem","text":"","code":""},{"path":"bayes-theorem.html","id":"examples","chapter":"Class 2 Bayes’ Theorem","heading":"2.1 Examples","text":"","code":""},{"path":"software.html","id":"software","chapter":"Class 3 Software","heading":"Class 3 Software","text":"","code":""},{"path":"software.html","id":"jags","chapter":"Class 3 Software","heading":"3.1 JAGS","text":"","code":"library(R2jags)\n\nmymodel= function(){\nfor ( i in 1:Ntotal ) {\n    y[i] ~ dbern( theta )\n  }\n  theta ~ dbeta( 1 , 1 )\n}\n\nn <- 50\ntheta.true <- .3\ny.sim <- rbinom(n, 1, theta.true)\n\nmydata = list(    \n  y = y.sim ,\n  Ntotal = n \n)\n\n\n\nfit <- jags(data=mydata, model=mymodel, parameters.to.save = c(\"theta\"), n.chain=2, n.iter=200, n.thin=1, n.burn=100)\nmodule glm loaded\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 50\n   Unobserved stochastic nodes: 1\n   Total graph size: 53\n\nInitializing model\nfit\nInference for Bugs model at \"/var/folders/fy/_0t49sys0713k84msqwk44fc0000gp/T//RtmpXLbqSR/model4cee6db30705.txt\", fit using jags,\n 2 chains, each with 200 iterations (first 100 discarded)\n n.sims = 200 iterations saved\n         mu.vect sd.vect   2.5%    25%    50%    75%  97.5%\ntheta      0.366   0.068  0.252  0.323  0.360  0.407  0.510\ndeviance  66.337   1.585 65.343 65.441 65.712 66.511 71.504\n          Rhat n.eff\ntheta    0.998   200\ndeviance 1.021   200\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 1.3 and DIC = 67.6\nDIC is an estimate of expected predictive error (lower deviance is better).library(R2jags)\n\nmodel= function(){\nM ~ dcat(p1)\n# sampling distribution is uniform over first N integers\n# use step function to change p1[j] to 0 for j>N\nfor (j in 1:1000) {\np1[j] <- step(N - j + 0.01)/N\n}\nN ~ dcat(p2)\nfor (j in 1:1000) {\nrecip[j] <- 1/j\np2[j] <- recip[j]/sum.recip\n}\nsum.recip <- sum(recip)\n}\ndata=list(\"M\"=100)\nfit <- jags(data=data, model=model,parameters.to.save=c(\"N\",\"M\"), n.chain=2, n.iter=100, n.thin=1, n.burn=0, DIC=FALSE)\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1\n   Unobserved stochastic nodes: 1\n   Total graph size: 7007\n\nInitializing model\nfit\nInference for Bugs model at \"/var/folders/fy/_0t49sys0713k84msqwk44fc0000gp/T//RtmpXLbqSR/model4cee11bcb869.txt\", fit using jags,\n 2 chains, each with 100 iterations (first 0 discarded)\n n.sims = 200 iterations saved\n  mu.vect sd.vect 2.5% 25%   50%    75%   97.5%  Rhat n.eff\nM 100.000   0.000  100 100 100.0 100.00 100.000 1.000     1\nN 264.665 196.366  101 127 184.5 321.75 836.275 0.997   200\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).library(rjags)\nmodel_string <- \"\nmodel {\nM ~ dcat(p1)\nfor (j in 1:1000) {\np1[j] <- (pow(j*step(N - j + 0.01)/N,2)-pow((j-1)*step(N - j  + 0.01)/N,2))\n}\nN ~ dcat(p2[])\nfor (j in 1:1000) {\nrecip[j] <- 1/j\np2[j] <- recip[j]/sum.recip\n}\nsum.recip <- sum(recip)\n}\"\n\ndata=list(\"M\"=50)\njmodel <- jags.model(textConnection(model_string), data = data,inits=list(N=100),n.chains = 2, n.adapt= 100,quiet=TRUE)\nupdate(jmodel, 10)\nmcmc_samples <- coda.samples(jmodel, variable.names=c(\"N\"), n.iter=100)\nsummary(mcmc_samples)\n\nIterations = 11:110\nThinning interval = 1 \nNumber of chains = 2 \nSample size per chain = 100 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean             SD       Naive SE Time-series SE \n       103.990         89.148          6.304          6.318 \n\n2. Quantiles for each variable:\n\n  2.5%    25%    50%    75%  97.5% \n 51.00  57.75  77.50 110.25 364.58 "}]
